- REMINDER: steps when setting up a network namespace and connecting to bridge:
	1. Create network namespace
	2. Create bridge network/interface
	3. Create VETH pairs (Pipe/Virtual Cable)
	4. Attach VETH (one end of the pipe) to namespace
	5. Attach other VETH (other end of the pipe) to bridge
	6. Assign IP addresses
	7. Bring the interfaces up
	8. Enable NAT - IP Masquerade

- REMINDER: steps when setting up a Docker and connecting to bridge:
	1. Create network namespace
	2. Create bridge network/interface
	3. Create VETH pairs (Pipe/Virtual Cable)
	4. Attach VETH (one end of the pipe) to namespace
	5. Attach other VETH (other end of the pipe) to bridge
	6. Assign IP addresses
	7. Bring the interfaces up
	8. Enable NAT - IP Masquerade

- This process is the same for most networking solutions

![[cni-1.png]]

- A standard defines how a program should look, how container runtimes should invoke them so that everyone can adhere to a single set of standards and develop solutions that work across runtimes
	- This is where Container Network Interface (CNI) comes in

- The CNI is a set of standards that define how programs should be developed to solve networking challenges in a container runtime environment
	- The programs are referred to as plugins

- The bridge program/script that connects a bridge to a network namespace is a CNI plugin

- CNI defines how plugins should be developed and how container runtimes should invoke them.

- CNI defines a set of responsibilities for container runtimes and plugins

- For container runtimes, CNI specifies that it is responsible for the following:

![[cni-2.png]]

- On the plugin side, CNI defines that the plugin should support the `add`, `del`, and `check` command line arguments and that these should accept parameters like container and network namespace

![[cni-3.png]]

- CNI comes with a set up supported plugins
	- Such as bridge, VLAN, IPVLAN, MACVLAN, and one for windows
	- Also IPAM plugins like host-local and DHCP
	- There are other plugins from third party organizations as well

- Docker does not implement CNI

- Docker has it's own set of networking standards known as CNM which stands for Container Network Model

- Due to the standard differences between CNI and CNM, the CNI plugins don't natively integrate with Docker
	- This doesn't mean you can't use Docker with CNI at all, it just means you have to workaround it yourself

- One of the ways to work around Docker and CNI is the way Kubernetes does it
	- This is by creating a Docker container without any networking configuration and manually invoking the bridge plugin

- When Kubernetes creates Docker containers, it creates them on the none network